{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effd13f9-a281-483b-9dea-059657e70b44",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632428f-d35e-4b9c-973e-c2fcf27fd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, log_loss, confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300536d-32a7-4df6-b753-e7815668e5fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62522d3-edaf-402f-bced-10e6e562762e",
   "metadata": {},
   "source": [
    "#### `limit_bal`: Amount of the given credit in dollars: it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "#### `sex`: Gender (1 = male; 2 = female).\n",
    "#### `education`: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "#### `marriage`: Marital status (1 = married; 2 = single; 3 = others).\n",
    "#### `age`: Age (year).\n",
    "#### `pay_1:pay_6`: History of past payment. We tracked the past monthly payment records (from April to September) as follows: `pay_1` = the repayment #### status in September; `pay_2` = the repayment status in August; . . .;`pay_6` = the repayment status in April. The measurement scale for the \n",
    "#### repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; #### 9 = payment delay for nine months and above.\n",
    "#### `bill_amt1:bill_amt6`: Amount of bill statement in dollars. `bill_amt1` = amount of bill statement in September; `bill_amt2` = amount of bill \n",
    "#### statement in August; . . .; `bill_amt6` = amount of bill statement in April.\n",
    "#### `pay_amt1:pay_amt6`: Amount of previous payment in dollars. `pay_amt1` = amount paid in September; `pay_amt2` = amount paid in August; . #### #### #### ...;`pay_amt6` = amount paid in April.\n",
    "#### `default_oct`: response in ${yes, no}$ of default in October"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187c92f-62eb-4e4a-b007-9d528dd55988",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d37af5-23cb-47e4-b549-6152cb5f06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(current_dir, \"copy of train.csv\"))\n",
    "train['default_oct'] = train['default_oct'].map({'no': 0, 'yes': 1}) # map to a numeric\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159cecb-d043-45cd-b83e-b75194b236c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(current_dir, \"copy of test.csv\"))\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255dfc9-ace0-4453-9b7c-7999aa8889e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f7874-1cd7-43c6-8d02-166cdcd48be0",
   "metadata": {},
   "source": [
    "#### `College` = 1 if undergrad or grad school, else 0\n",
    "#### `Marriage` = 1 if married, else 0\n",
    "#### `max_credit_utilization` = max(bill_amt1, ... , bill_amt6) / limit_bal\n",
    "#### `sum_missed_months` = sum(pay_1, ... , pay_6)\n",
    "#### `max_missed_months` = max(pay_1, ... , pay_6)\n",
    "#### `max_payment` = max(pay_amt1, ... , pay_amt6)\n",
    "#### `mean_payment` = mean(pay_amt1, ... , pay_amt6)  for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa13f3-cf9d-45fd-866e-0293d0ffe31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['College'] = train['education'].apply(lambda x: 1 if x in [1, 2] else 0)\n",
    "train['Marriage'] = train['marriage'].apply(lambda x: 1 if x in [1] else 0)\n",
    "train['max_credit_utilization'] = train[['bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6']].max(axis=1) / train['limit_bal']\n",
    "train['sum_missed_months'] = train[['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']].sum(axis=1)\n",
    "train['max_missed_months'] = train[['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']].max(axis=1)\n",
    "train['max_payment'] = train[['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']].max(axis=1)\n",
    "train['mean_payment'] = train[['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']].mean(axis=1)\n",
    "\n",
    "test['College'] = test['education'].apply(lambda x: 1 if x in [1, 2] else 0)\n",
    "test['Marriage'] = test['marriage'].apply(lambda x: 1 if x in [1] else 0)\n",
    "test['max_credit_utilization'] = test[['bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6']].max(axis=1) / test['limit_bal']\n",
    "test['sum_missed_months'] = test[['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']].sum(axis=1)\n",
    "test['max_missed_months'] = test[['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']].max(axis=1)\n",
    "test['max_payment'] = test[['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']].max(axis=1)\n",
    "test['mean_payment'] = test[['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']].mean(axis=1)\n",
    "test['education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e82af-3112-4e95-a20a-3e4b1ac5a9f0",
   "metadata": {},
   "source": [
    "# handle missing data in pay_5, pay_6, bill_amt5, bill_amt6, pay_amt5, pay_amt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564dd58-a835-4a1d-9b7c-0d7dc6cedf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations show:\n",
    "# pay_5, pay_6, bill_amt5, bill_amt6 correlate best to their prior time period\n",
    "# pay_amt5, pay_amt6 correlate best to mean_payment\n",
    "correlations = train.corr()[['pay_5', 'pay_6', 'bill_amt5', 'bill_amt6', 'pay_amt5', 'pay_amt6']].sort_values(by='bill_amt5', ascending=False)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b98a82-6dfb-48bc-85b0-678f6fa0186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build regressor imputations based on prior time period available\n",
    "# e.g.  if pay_5 is null, impute from pay_4\n",
    "#                 then\n",
    "#       if pay_6 is null, impute from pay_5\n",
    "\n",
    "# remove pay nulls before training\n",
    "train_pay5 = train[train['pay_5'].notnull()]\n",
    "train_pay6 = train[train['pay_6'].notnull() & train['pay_5'].notnull()]\n",
    "# remove bill_amt nulls before training\n",
    "train_bill_amt5 = train[train['bill_amt5'].notnull()]\n",
    "train_bill_amt6 = train[train['bill_amt6'].notnull() & train['bill_amt5'].notnull()]\n",
    "\n",
    "# pay regressors\n",
    "train_on = train_pay5\n",
    "regressor_pay5 = LinearRegression()\n",
    "regressor_pay5.fit(train_on[['pay_4']], train_on[['pay_5']])\n",
    "train_on = train_pay6\n",
    "regressor_pay6 = LinearRegression()\n",
    "regressor_pay6.fit(train_on[['pay_5']], train_on[['pay_6']])\n",
    "#bill_amt regressors\n",
    "train_on = train_bill_amt5\n",
    "regressor_bill_amt5 = LinearRegression()\n",
    "regressor_bill_amt5.fit(train_on[['bill_amt4']], train_on[['bill_amt5']])\n",
    "train_on = train_bill_amt6\n",
    "regressor_bill_amt6 = LinearRegression()\n",
    "regressor_bill_amt6.fit(train_on[['bill_amt5']], train_on[['bill_amt6']])\n",
    "\n",
    "\n",
    "# impute for pay5\n",
    "missing_pay5 = train[train['pay_5'].isnull()]\n",
    "predicted_pay5 = regressor_pay5.predict(missing_pay5[['pay_4']])\n",
    "train.loc[train['pay_5'].isnull(), 'pay_5'] = predicted_pay5\n",
    "missing_pay5 = test[test['pay_5'].isnull()]\n",
    "predicted_pay5 = regressor_pay5.predict(missing_pay5[['pay_4']])\n",
    "test.loc[test['pay_5'].isnull(), 'pay_5'] = predicted_pay5\n",
    "# impute for pay6\n",
    "missing_pay6 = train[train['pay_6'].isnull()]\n",
    "predicted_pay6 = regressor_pay6.predict(missing_pay6[['pay_5']])\n",
    "train.loc[train['pay_6'].isnull(), 'pay_6'] = predicted_pay6\n",
    "missing_pay6 = test[test['pay_6'].isnull()]\n",
    "predicted_pay6 = regressor_pay6.predict(missing_pay6[['pay_5']])\n",
    "test.loc[test['pay_6'].isnull(), 'pay_6'] = predicted_pay6\n",
    "# impute for bill_amt5\n",
    "missing_bill_amt5 = train[train['bill_amt5'].isnull()]\n",
    "predicted_bill_amt5 = regressor_bill_amt5.predict(missing_bill_amt5[['bill_amt4']])\n",
    "train.loc[train['bill_amt5'].isnull(), 'bill_amt5'] = predicted_bill_amt5\n",
    "missing_bill_amt5 = test[test['bill_amt5'].isnull()]\n",
    "predicted_bill_amt5 = regressor_bill_amt5.predict(missing_bill_amt5[['bill_amt4']])\n",
    "test.loc[test['bill_amt5'].isnull(), 'bill_amt5'] = predicted_bill_amt5\n",
    "# impute for bill_amt6\n",
    "missing_bill_amt6 = train[train['bill_amt6'].isnull()]\n",
    "predicted_bill_amt6 = regressor_bill_amt6.predict(missing_bill_amt6[['bill_amt5']])\n",
    "train.loc[train['bill_amt6'].isnull(), 'bill_amt6'] = predicted_bill_amt6\n",
    "missing_bill_amt6 = test[test['bill_amt6'].isnull()]\n",
    "predicted_bill_amt6 = regressor_bill_amt6.predict(missing_bill_amt6[['bill_amt5']])\n",
    "test.loc[test['bill_amt6'].isnull(), 'bill_amt6'] = predicted_bill_amt6\n",
    "\n",
    "# replace pay_amt missing values with mean of other pay_amt's\n",
    "train['pay_amt5'].fillna(train['mean_payment'], inplace=True)\n",
    "train['pay_amt6'].fillna(train['mean_payment'], inplace=True)\n",
    "test['pay_amt5'].fillna(test['mean_payment'], inplace=True)\n",
    "test['pay_amt6'].fillna(test['mean_payment'], inplace=True)\n",
    "\n",
    "# test.info()  # check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02cd9d-1a6f-470f-825c-d23d94879050",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdaf48-26ad-4089-9762-86b94d0b48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at correlations to default_oct\n",
    "correlations = train.corr()[['default_oct']].sort_values(by='default_oct', ascending=False)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0085f5-1a93-429d-b977-8b9e8b8ee1f6",
   "metadata": {},
   "source": [
    "# evaluate different modeling methods\n",
    "### using log loss, test logistic regression, random forest, gradient boosted classifier, xgboost and neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d23c9-b932-45aa-a767-2c4322e3eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## used correlations(for importance/removing multicolinearity) and feature importance from tree methods to trim down feature set.\n",
    "# X = train.drop(columns=['default_oct','customer_id','bill_amt6','bill_amt5','bill_amt4','bill_amt2','bill_amt3','College','marriage']) \n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# y = train['default_oct']   \n",
    "\n",
    "# models = {\n",
    "#     'Logistic Regression Lasso': LogisticRegression(penalty='l1', solver='liblinear', random_state=27),\n",
    "#     'Logistic Regression Ridge': LogisticRegression(penalty='l2', random_state=27),\n",
    "#     'Random Forest': RandomForestClassifier(random_state=27),\n",
    "#     'Gradient Boosting': GradientBoostingClassifier(random_state=27),\n",
    "#     'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=27),\n",
    "#     'Neural Network': MLPClassifier(max_iter=1000, random_state=27)\n",
    "# }\n",
    "\n",
    "# param_grids = {\n",
    "#     'Logistic Regression Lasso': {\n",
    "#         'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "#         'max_iter': [100, 300, 500, 700]\n",
    "#     },\n",
    "#     'Logistic Regression Ridge': {\n",
    "#         'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "#         'solver': ['lbfgs', 'saga'],\n",
    "#         'max_iter': [100, 300, 500, 700]\n",
    "#     },\n",
    "#     'Random Forest': {\n",
    "#         'n_estimators': [25, 50, 75, 100],\n",
    "#         'max_depth': [5, 15, 25, 35],\n",
    "#         'min_samples_split': [50, 100, 200, 300, 400]\n",
    "#     },\n",
    "#     'Gradient Boosting': {\n",
    "#         'n_estimators': [25, 50, 100, 200],\n",
    "#         'learning_rate': [.01, .1, .2, .3, .4],\n",
    "#         'max_depth': [3, 6, 9]\n",
    "#     },\n",
    "#     'XGBoost': {\n",
    "#         'n_estimators': [50, 100, 200, 300, 400],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'max_depth': [3, 6, 9],\n",
    "#         'subsample': [0.5, 0.7, 1]\n",
    "#     },\n",
    "#     'Neural Network': {\n",
    "#         'hidden_layer_sizes': [(50,), (100,), (50, 50), (100,100)],\n",
    "#         'activation': ['tanh', 'relu', 'logistic'],\n",
    "#         'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "#         'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# def evaluate_models(models, param_grids, X, y):\n",
    "#     results = {}\n",
    "#     best_models = {}\n",
    "#     cross_val_preds = {}\n",
    "#     cross_val_probs = {}\n",
    "#     log_loss_scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "#     kf = KFold(n_splits=3, shuffle=True, random_state=27)\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         grid_search = GridSearchCV(model, param_grids[name], cv=kf, verbose=2, n_jobs=-1, scoring=log_loss_scorer) \n",
    "#         grid_search.fit(X, y)\n",
    "\n",
    "#         # best model from grid search\n",
    "#         best_model = grid_search.best_estimator_\n",
    "#         cross_val_pred = cross_val_predict(best_model, X, y, cv=kf, n_jobs=-1)\n",
    "#         cross_val_prob = cross_val_predict(best_model, X, y, cv=kf, method='predict_proba', n_jobs=-1)\n",
    "#         score = log_loss(y,cross_val_prob[:,1])\n",
    "#         results[name] = score\n",
    "#         print(f\"{name}: Log Loss = {score:.3f}\")\n",
    "        \n",
    "#         best_model.fit(X, y)\n",
    "#         best_models[name] = best_model\n",
    "#         cross_val_preds[name] = cross_val_pred\n",
    "#         cross_val_probs[name] = cross_val_prob\n",
    "#     return results, best_models, cross_val_preds, cross_val_probs\n",
    "\n",
    "# results, best_models, cross_val_preds, cross_val_probs = evaluate_models(models, param_grids, X, y)\n",
    "# for model_name, score in results.items():\n",
    "#     print(f\"{model_name}: Log Loss = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa89a18-5dbf-4457-a629-57d922383813",
   "metadata": {},
   "source": [
    "##### `Logistic Regression Lasso`: Log Loss = .462\n",
    "##### `Logistic Regression Ridge`: Log Loss = .462\n",
    "##### *** `Random Forest`: *** Log Loss = .429 top ranked, lowest log loss\n",
    "##### *** `Gradient Boosting`: *** Log Loss = .429 top ranked, lowest log loss\n",
    "##### *** `XGBoost`: *** Log Loss = .430 top ranked, second lowest log loss\n",
    "##### `Neural Network`: Log Loss = .434 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c222fbe-181d-415e-a71a-e1ed534ed704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## used to retrieve hyperparameters selected from grid search above\n",
    "# for model_name, model in best_models.items():\n",
    "#     print(model_name, \"Hyperparameters: \\n\",model.get_params(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04689f56-8510-4b9e-999b-79811d34104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine what mix of best performing models makes the best soft voting classifier\n",
    "# X = train.drop(columns=['default_oct','customer_id','bill_amt6','bill_amt5','bill_amt4','bill_amt2','bill_amt3','College','marriage']) \n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# y = train['default_oct']   \n",
    "\n",
    "# kf = KFold(n_splits=4, shuffle=True, random_state=27) \n",
    "# xgb = XGBClassifier(n_estimators=300,max_depth=6,subsample=1,learning_rate= 0.01,random_state=27)\n",
    "# rf = RandomForestClassifier(max_depth=35, min_samples_split = 200, n_estimators= 100, random_state=27)\n",
    "# gb = GradientBoostingClassifier(n_estimators=25, learning_rate=0.2, max_depth=3, random_state=27)\n",
    "# nn = MLPClassifier(hidden_layer_sizes=(100,),activation='tanh',learning_rate='constant',learning_rate_init=0.001, alpha=0.1, max_iter=1000, random_state=27)\n",
    "\n",
    "# # Soft Voting Classifier with Random Forest, Gradient Boosting, XGBoost, and Neural Network \n",
    "# sv0 = VotingClassifier(\n",
    "#     estimators=[('RF',rf),('GB',gb),('XGB',xgb),('NN',nn)],\n",
    "#     voting='soft'\n",
    "# )\n",
    "# sv0_prob = cross_val_predict(sv0, X, y, cv=kf, method='predict_proba', n_jobs=-1)\n",
    "# sv0_log_loss = log_loss(y, sv0_prob)\n",
    "# print(f\" SV0 - Log Loss: {sv0_log_loss:.5f}\") \n",
    "# # Soft Voting Classifier with Random Forest, Gradient Boosting, and XGBoost\n",
    "# sv1 = VotingClassifier(\n",
    "#     estimators=[('RF',rf),('GB',gb),('XGB',xgb)],\n",
    "#     voting='soft'\n",
    "# )\n",
    "# sv1_prob = cross_val_predict(sv1, X, y, cv=kf, method='predict_proba', n_jobs=-1)\n",
    "# sv1_log_loss = log_loss(y, sv1_prob)\n",
    "# print(f\" SV1 - Log Loss: {sv1_log_loss:.5f}\") \n",
    "# # Soft Voting Classifier with Random Forest and Gradient Boosting\n",
    "# sv2 = VotingClassifier(\n",
    "#     estimators=[('RF',rf),('GB',gb)],\n",
    "#     voting='soft'\n",
    "# )\n",
    "# sv2_prob = cross_val_predict(sv2, X, y, cv=kf, method='predict_proba', n_jobs=-1)\n",
    "# sv2_log_loss = log_loss(y, sv2_prob)\n",
    "# print(f\" SV2 - Log Loss: {sv2_log_loss:.5f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c6b3b-bb48-4f78-8568-c3eca4f97932",
   "metadata": {},
   "source": [
    "##### `Voting With RF, GB, XGB, NN`: Log Loss = 0.42789\n",
    "##### `Voting With RF, GB, XGB`: Log Loss = 0.42779 *** top ranked, lowest log loss ***\n",
    "##### `Voting With RF, GB`: Log Loss = 0.42814 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a9219-1b31-49fc-9e98-be0f751f8509",
   "metadata": {},
   "source": [
    "# final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bf491-d89d-4456-8aa0-25a668b9c2cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=['default_oct','customer_id','bill_amt6','bill_amt5','bill_amt4','bill_amt2','bill_amt3','College','marriage']) \n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = train['default_oct']   \n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=27) \n",
    "xgb = XGBClassifier(n_estimators=300,max_depth=6,subsample=1,learning_rate= 0.01,random_state=27)\n",
    "rf = RandomForestClassifier(max_depth=35, min_samples_split = 200, n_estimators= 100, random_state=27)\n",
    "gb = GradientBoostingClassifier(n_estimators=25, learning_rate=0.2, max_depth=3, random_state=27)\n",
    "\n",
    "soft_voting_model = VotingClassifier(\n",
    "    estimators=[('RF',rf),('GB',gb),('XGB',xgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# confusion matrix using cross validation predictions\n",
    "sv_pred = cross_val_predict(soft_voting_model, X, y, cv=kf, n_jobs=-1)\n",
    "plt.figure(figsize=(1, .5))\n",
    "cm = confusion_matrix(train['default_oct'], sv_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Soft Voting Confusion Matrix - Log Loss = 0.42779')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783a713-5135-4c3d-bf33-b300db7d96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749afcfc-dcce-4ab7-98b3-05f24cbcdd19",
   "metadata": {},
   "source": [
    "# final model prediction export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35a147-f7f2-429d-8d2e-2a82dec539df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest log loss found with voting classifier\n",
    "X = test.drop(columns=['customer_id','bill_amt6','bill_amt5','bill_amt4','bill_amt2','bill_amt3','College','marriage']) \n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "prob = soft_voting_model.predict_proba(X)[:, 1] \n",
    "#customer_id, pr_y is expected output\n",
    "df = pd.concat([test[['customer_id']], pd.DataFrame(prob)], axis=1)\n",
    "df = df.rename(columns={0: 'pr_y'})\n",
    "df.to_csv('Norm_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
